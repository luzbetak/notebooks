{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfda871d-c598-4bf2-8fcc-cfe8fb1b7e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting faker\n  Downloading Faker-18.6.0-py3-none-any.whl (1.7 MB)\nRequirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.9/site-packages (from faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\nInstalling collected packages: faker\nSuccessfully installed faker-18.6.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23fb8e9-d774-474c-b248-022e3615e09b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, DateType, StructType, StructField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876208ae-ac9f-4bec-9c47-87d1525204b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Generate fake products and sales data\n",
    "fake = Faker()\n",
    "\n",
    "def custom_sku():\n",
    "    return f\"{fake.random_number(digits=4)}-{fake.random_number(digits=4)}-{fake.random_number(digits=4)}\"\n",
    "\n",
    "def generate_products(n):\n",
    "    products = []\n",
    "    for _ in range(n):\n",
    "        product = { \n",
    "            'product_id': custom_sku(),\n",
    "            'name': fake.bs(),\n",
    "            'description': fake.sentence(),\n",
    "            'price': round(random.uniform(1, 500), 2), \n",
    "            'currency': fake.currency_code(),\n",
    "            'barcode': fake.ean()\n",
    "        }   \n",
    "        products.append(product)\n",
    "    return products\n",
    "\n",
    "\n",
    "def generate_sales(products, n): \n",
    "    sales = []\n",
    "    for _ in range(n):\n",
    "        product = random.choice(products)\n",
    "        sale_date = fake.date_between(start_date='-1y', end_date='today')\n",
    "        sale = { \n",
    "            'product_id': product['product_id'],\n",
    "            'quantity': random.randint(1, 10),\n",
    "            'total_price': product['price'] * random.randint(1, 10),\n",
    "            'sale_date': sale_date\n",
    "        }   \n",
    "        sales.append(sale)\n",
    "    return sales\n",
    "\n",
    "products = generate_products(100)\n",
    "sales    = generate_sales(products, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4133687-ec82-45d4-8bf2-b5a959dca58a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Convert Python data to PySpark DataFrames\n",
    "\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField('product_id',   StringType(), True),\n",
    "    StructField('name',         StringType(), True),\n",
    "    StructField('description',  StringType(), True),\n",
    "    StructField('price',        FloatType(), True),\n",
    "    StructField('currency',     StringType(), True),\n",
    "    StructField('barcode',      StringType(), True)\n",
    "])\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField('product_id',   StringType(), True),\n",
    "    StructField('quantity',     IntegerType(), True),\n",
    "    StructField('total_price',  FloatType(), True),\n",
    "    StructField('sale_date',    DateType(), True)\n",
    "])\n",
    "\n",
    "products_df = spark.createDataFrame(products, schema=products_schema)\n",
    "sales_df = spark.createDataFrame(sales, schema=sales_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c7c6f0-ceb3-46d0-a7e8-79ca3fcf85f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Save DataFrames as tables\n",
    "table_catalog = \"development\"\n",
    "table_schema  = \"node_process_integrated\"\n",
    "products      = f\"{table_catalog}.{table_schema}.products\"\n",
    "sales         = f\"{table_catalog}.{table_schema}.sales\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS  {products}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS  {sales}\")\n",
    "\n",
    "products_df.write.mode(\"overwrite\").saveAsTable(products)\n",
    "sales_df.write.mode(\"overwrite\").saveAsTable(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66808a6-c1dd-4c69-875a-c2d2dc5e26de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------------------+--------------------------------------------------+------+--------+-------------+\n|product_id    |name                                |description                                       |price |currency|barcode      |\n+--------------+------------------------------------+--------------------------------------------------+------+--------+-------------+\n|7603-6087-4291|mesh plug-and-play bandwidth        |Exist old thus evening cultural environment.      |74.18 |CZK     |4323774350826|\n|8945-20-4410  |cultivate holistic e-markets        |Plant hit talk good finally traditional.          |395.63|JPY     |4016429866956|\n|8348-3834-4602|seize mission-critical supply-chains|Available arm recently fine bag learn goal expert.|70.53 |COP     |3278629469585|\n|840-4110-717  |syndicate ubiquitous e-tailers      |Important fund spend exactly rule.                |288.75|TWD     |9117976204347|\n|1129-3124-7918|drive collaborative channels        |Raise teacher explain.                            |474.32|UAH     |6030882343821|\n+--------------+------------------------------------+--------------------------------------------------+------+--------+-------------+\n\n+--------------+--------+-----------+----------+\n|product_id    |quantity|total_price|sale_date |\n+--------------+--------+-----------+----------+\n|378-3378-6676 |5       |1443.72    |2022-08-17|\n|269-1174-1908 |5       |2773.89    |2022-05-28|\n|5612-947-566  |5       |1497.33    |2022-05-01|\n|3189-5902-1458|5       |1397.95    |2022-08-01|\n|742-1528-8175 |3       |1316.8     |2023-01-05|\n+--------------+--------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Display data from sales table\n",
    "table_catalog = \"development\"\n",
    "table_schema  = \"node_process_integrated\"\n",
    "products      = f\"{table_catalog}.{table_schema}.products\"\n",
    "sales         = f\"{table_catalog}.{table_schema}.sales\"\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {products} ORDER BY RAND() LIMIT 5\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {sales}    ORDER BY RAND() LIMIT 5\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Product-Sales-Datasets",
   "notebookOrigID": 1801714774888645,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
